## 问题

- 因果注意力可以让当前词看不到后面的词，但是那些非因果注意力模块中当前词可以看到后面的词吗，可以的话那这个模型整体当前词还是可以获取后面的词的信息吧
- GPT 中限制推理长度的因素
    - pos_embedding 的构造需要提前获取 seq_len
    - attention 中 mask 如果静态构造也需要提前获取 seq_len(动态构造可规避此问题)