## 问题

- 因果注意力可以让当前词看不到后面的词，但是那些"非因果注意力"模块中当前词可以看到后面的词吗，可以的话那这个模型整体当前词还是可以获取后面的词的信息吧(未泄露: 当前词的生成仅依赖于上一层的当前位置的词和这个位置之前的词)
- GPT 中限制推理长度的因素
    - pos_embedding 的构造需要提前获取 seq_len
    - attention 中 mask 如果静态构造也需要提前获取 seq_len(动态构造可规避此问题)

## Todo

- 训练一份勉强能看的 GPT 权重
- Temperature + topK(P138)
- 分类微调(P156)
- SFT微调(P185)
- LoRA 微调(P282 P294)